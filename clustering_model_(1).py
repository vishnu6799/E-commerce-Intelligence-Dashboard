# -*- coding: utf-8 -*-
"""Clustering_Model (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pPO4-HYzcX19Nl6hQZfUaSQSUXj_FCRE
"""

import numpy as np
import pandas as pd

train_df = pd.read_csv("train_data.csv")
train_df.head(10)

train_df.info()

train_df.isnull().sum()

test_df = pd.read_csv("test_data.csv")
test_df.head(10)

test_df.info()

test_df.isnull().sum()

cols_to_drop = ['year', 'month', 'day', 'session_id', 'country', 'page2_clothing_model']
train_df_clust = train_df.drop(columns=cols_to_drop)

# Recode price_2: 1 (above average) -> 1, 2 (below average) -> 0
train_df_clust['price_2'] = train_df_clust['price_2'].replace({2: 0})

train_df_clust.head()

train_df_clust.info()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_train = scaler.fit_transform(train_df_clust)

scaled_train_df = pd.DataFrame(scaled_train, columns=train_df_clust.columns)

scaled_train_df

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

sse = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_train)
    sse.append(kmeans.inertia_)

# Plot Elbow Curve
plt.figure(figsize=(8, 4))
plt.plot(K, sse, 'bo-', linewidth=2)
plt.xlabel('Number of clusters (k)')
plt.ylabel('SSE (Inertia)')
plt.title('Elbow Method For Optimal k')
plt.grid(True)
plt.show()

from sklearn.cluster import KMeans

# Fit the model
kmeans = KMeans(n_clusters=4, random_state=42)
train_df_clust['cluster'] = kmeans.fit_predict(scaled_train)

# Basic profiling
cluster_profile = train_df_clust.groupby('cluster').mean().round(2)
print(cluster_profile)

print(train_df_clust['cluster'].value_counts())

# Drop irrelevant columns
cols_to_drop = ['year', 'month', 'day', 'session_id', 'country', 'page2_clothing_model']
test_df_clust = test_df.drop(columns=cols_to_drop)

# Recode price_2: 1 -> 1, 2 -> 0
test_df_clust['price_2'] = test_df_clust['price_2'].replace({2: 0})

# Scale using the same scaler fitted on train data
scaled_test = scaler.transform(test_df_clust)

# Predict clusters using trained KMeans model
test_df_clust['cluster'] = kmeans.predict(scaled_test)

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Reduce to 2D
pca = PCA(n_components=2)
components = pca.fit_transform(scaled_train)

# Plot
plt.figure(figsize=(8, 6))
scatter = plt.scatter(components[:, 0], components[:, 1],
                      c=train_df_clust['cluster'], cmap='Set1', alpha=0.6)
plt.title('Customer Segments Visualized (Train Data)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.grid(True)
plt.show()

from sklearn.metrics import calinski_harabasz_score
ch_score = calinski_harabasz_score(scaled_train, train_df_clust['cluster'])
print("Calinski-Harabasz Score:", ch_score)

from sklearn.metrics import davies_bouldin_score
db_score = davies_bouldin_score(scaled_train, train_df_clust['cluster'])
print("Davies-Bouldin Score:", db_score)

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# DBSCAN uses Îµ-neighborhoods and density
dbscan = DBSCAN(eps=1.5, min_samples=5)  # eps needs tuning
db_labels = dbscan.fit_predict(scaled_train)

# Add to dataframe
train_df_clust['dbscan_cluster'] = db_labels

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA (2 components for 2D plot)
pca = PCA(n_components=2)
pca_components = pca.fit_transform(scaled_train)

# DBSCAN plot
plt.figure(figsize=(8, 6))
scatter = plt.scatter(pca_components[:, 0], pca_components[:, 1],
                      c=db_labels, cmap='tab10', alpha=0.6)
plt.title('DBSCAN Clustering Visualization (PCA-reduced)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.grid(True)
plt.show()

from sklearn.metrics import calinski_harabasz_score
ch_score = calinski_harabasz_score(scaled_train, train_df_clust['dbscan_cluster'])
print("Calinski-Harabasz Score:", ch_score)

from sklearn.metrics import davies_bouldin_score
db_score = davies_bouldin_score(scaled_train, train_df_clust['dbscan_cluster'])
print("Davies-Bouldin Score:", db_score)

from sklearn.mixture import GaussianMixture

# Fit with same number of clusters as KMeans
gmm = GaussianMixture(n_components=4, random_state=42)
gmm_labels = gmm.fit_predict(scaled_train)

# Add to dataframe
train_df_clust['gmm_cluster'] = gmm_labels

# GMM plot
plt.figure(figsize=(8, 6))
scatter = plt.scatter(pca_components[:, 0], pca_components[:, 1],
                      c=gmm_labels, cmap='tab10', alpha=0.6)
plt.title('GMM Clustering Visualization (PCA-reduced)')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.grid(True)
plt.show()

from sklearn.metrics import calinski_harabasz_score
ch_score = calinski_harabasz_score(scaled_train, train_df_clust['gmm_cluster'])
print("Calinski-Harabasz Score:", ch_score)

from sklearn.metrics import davies_bouldin_score
db_score = davies_bouldin_score(scaled_train, train_df_clust['gmm_cluster'])
print("Davies-Bouldin Score:", db_score)

import joblib

# Save the model
joblib.dump(kmeans, 'clustering_model.pkl')

