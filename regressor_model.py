# -*- coding: utf-8 -*-
"""Regressor Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sXizAkEMGCdEzJgz8foFOuzhcqU4TlmN
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt
from sklearn.cluster import KMeans

# Load and preprocess data
df = pd.read_csv("train_data.csv")
df = df.drop(columns=['page2_clothing_model'])

unique_sessions = df['session_id'].unique()
train_sessions, test_sessions = train_test_split(unique_sessions, test_size=0.2, random_state=42)

train_df = df[df['session_id'].isin(train_sessions)].reset_index(drop=True)
test_df = df[df['session_id'].isin(test_sessions)].reset_index(drop=True)

train_df = train_df.drop(columns='session_id')
test_df = test_df.drop(columns='session_id')

train_df = train_df.drop(columns='price_2')
test_df = test_df.drop(columns='price_2')

X_train = train_df.drop(columns='price')
y_train = train_df['price']

X_test = test_df.drop(columns='price')
y_test = test_df['price']

# Identify numerical and categorical columns
# Assuming all remaining columns are numerical, as per the original code
num_cols = X_train.columns.tolist()

# Create preprocessing pipelines
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

preprocessor = ColumnTransformer([
    ('num', num_pipeline, num_cols)
])

# Build and train the regression pipeline
reg_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', LinearRegression())
])

reg_pipeline.fit(X_train, y_train)

# Evaluate the pipeline
y_pred = reg_pipeline.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("=== Linear Regression Pipeline Evaluation ===")
print(f"MAE : {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R²  : {r2:.4f}")

# Train and evaluate multiple models
# Note: StandardScaler is applied manually before the loop for linear models
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(alpha=1.0),
    "Lasso Regression": Lasso(alpha=0.1),
    "Decision Tree": DecisionTreeRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "AdaBoost": AdaBoostRegressor(n_estimators=100, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=100, random_state=42, verbosity=0),
    "LightGBM": LGBMRegressor(n_estimators=100, random_state=42, verbose=-1)
}

for name, model in models.items():
    if "Regression" in name:
        model.fit(X_train_scaled, y_train)
        preds = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        preds = model.predict(X_test)

    mae = mean_absolute_error(y_test, preds)
    rmse = mean_squared_error(y_test, preds, squared=False)
    r2 = r2_score(y_test, preds)

    print(f"\n=== {name} ===")
    print(f"MAE : {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R²  : {r2:.4f}")

# Save the best model and the preprocessor
xgb = models["XGBoost"]
joblib.dump(xgb, "xgb_model.pkl")

# Save the preprocessor used for the regressor pipeline
joblib.dump(preprocessor, "preprocessor.pkl")

# The code for classification and clustering models should also be included here
# to generate 'best_classifier_model.pkl' and 'clustering_model.pkl'
# For example, for clustering
# kmeans_model = KMeans(n_clusters=3, random_state=42)
# kmeans_model.fit(preprocessor.fit_transform(X_train))
# joblib.dump(kmeans_model, 'clustering_model.pkl')